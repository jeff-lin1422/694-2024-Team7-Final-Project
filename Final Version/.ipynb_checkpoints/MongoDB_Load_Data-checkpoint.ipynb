{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient \n",
    "#with open('C:/Users/sahit/Desktop/MSDS/Database/HW2/Connection', 'r') as file:\n",
    "    # Read the entire contents of the file\n",
    "    #MONGODB_URI = file.read()\n",
    "#     print(MONGODB_URI)\n",
    "#client = MongoClient(MONGODB_URI)\n",
    "\n",
    "# db = client.mydatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "# client = pymongo.MongoClient()  \n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client.db \n",
    "tweets_collection = db.tweets  \n",
    "hashtags_collection = db.hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part contains code to store data, because we already stored it, we commented it out so it won't run again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def insert_tweet_info(tweet_info):\n",
    "    \"\"\"\n",
    "    Insert tweet information into the tweets collection in MongoDB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tweets_collection.insert_one(tweet_info)\n",
    "    except errors.DuplicateKeyError:\n",
    "        print(\"Tweet already exists:\", tweet_info['id'])\n",
    "    except Exception as e:\n",
    "        print(\"Error inserting tweet:\", e)\n",
    "\n",
    "with open(\"corona-out-3\", \"r\") as f1:\n",
    "    for line in f1:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            if data['text'].startswith('RT'):\n",
    "                original_tweet = data['retweeted_status']\n",
    "                tweet_info = {\n",
    "                    'created_at': data['created_at'],\n",
    "                    'retweeted_id': data['id'],\n",
    "                    'og_tweet_id': original_tweet['id'], \n",
    "                    'text': data['text'],\n",
    "                    'screen_name': data['user']['screen_name'],\n",
    "                    'retweet_count': data['retweet_count'],\n",
    "                    'favorite_count': data['favorite_count'],\n",
    "                    'lang': data['lang'],\n",
    "                    'quote_count': data['quote_count'],\n",
    "                    'reply_count': data['reply_count'],\n",
    "                    'media': ','.join([media['media_url'] for media in data.get('entities', {}).get('media', [])]) if 'media' in data.get('entities', {}) else None,\n",
    "                    'hashtags': [hashtag[\"text\"] for hashtag in data['retweeted_status']['extended_tweet']['entities']['hashtags']] if 'retweeted_status' in data and 'extended_tweet' in data['retweeted_status']['entities'] else [],\n",
    "                    'is_retweet': True\n",
    "                }\n",
    "                \n",
    "                original_tweet_info = {\n",
    "                    'created_at': original_tweet['created_at'],\n",
    "                    'retweeted_id': 0,\n",
    "                    'og_tweet_id': original_tweet['id'],\n",
    "                    'text': original_tweet['text'],\n",
    "                    'screen_name': original_tweet['user']['screen_name'],\n",
    "                    'retweet_count': original_tweet['retweet_count'],\n",
    "                    'favorite_count': original_tweet['favorite_count'],\n",
    "                    'lang': original_tweet['lang'],\n",
    "                    'quote_count': original_tweet['quote_count'],\n",
    "                    'reply_count': original_tweet['reply_count'],\n",
    "                    'media': ','.join([media['media_url'] for media in original_tweet.get('entities', {}).get('media', [])]) if 'media' in original_tweet.get('entities', {}) else None,\n",
    "                    'hashtags': [hashtag[\"text\"] for hashtag in original_tweet['entities']['hashtags']] if 'entities' in original_tweet and 'hashtags' in original_tweet['entities'] else [],\n",
    "                    'is_retweet': False  # Original tweet, not a retweet\n",
    "                }\n",
    "                insert_tweet_info(tweet_info)\n",
    "                orig_query = {'$and': [{'og_tweet_id': original_tweet['id']},{'retweeted_id': 0}]}\n",
    "                existing_doc = tweets_collection.find_one(orig_query)\n",
    "                if existing_doc:\n",
    "                    continue\n",
    "                else:\n",
    "                    insert_tweet_info(original_tweet_info)\n",
    "            else:\n",
    "                tweet_info = {\n",
    "                    'created_at': data['created_at'],\n",
    "                    'retweeted_id': 0,\n",
    "                    'og_tweet_id': data['id'],\n",
    "                    'text': data['text'],\n",
    "                    'screen_name': data['user']['screen_name'],\n",
    "                    'retweet_count': data['retweet_count'],\n",
    "                    'favorite_count': data['favorite_count'],\n",
    "                    'lang': data['lang'],                \n",
    "                    'quote_count': data['quote_count'],\n",
    "                    'reply_count': data['reply_count'],\n",
    "                    'media': ','.join([media['media_url'] for media in data.get('entities', {}).get('media', [])]) if 'media' in data.get('entities', {}) else None,\n",
    "                    'hashtags': [hashtag[\"text\"] for hashtag in data['entities']['hashtags']] if 'entities' in data and 'hashtags' in data['entities'] else [],\n",
    "                    'is_retweet': False  # Not a retweet\n",
    "                }\n",
    "                insert_tweet_info(tweet_info)\n",
    "        except Exception as e:\n",
    "            print(line)\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 112036\n"
     ]
    }
   ],
   "source": [
    "collection = tweets_collection  # Replace 'tweets' with your actual collection name\n",
    "\n",
    "# Count the number of documents in the collection\n",
    "num_documents = collection.count_documents({})\n",
    "\n",
    "# Print the number of documents\n",
    "print(\"Number of documents:\", num_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n"
     ]
    }
   ],
   "source": [
    "orig_query = {'$and': [{'og_tweet_id': 1253992905703862272},{'retweeted_id': 0}]}\n",
    "existing_doc = tweets_collection.find_one(orig_query)\n",
    "if existing_doc:\n",
    "    print(\"found\")\n",
    "else:\n",
    "    print(\"insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes in a dictionary and converts it to a list\n",
    "'''\n",
    "def dict_to_list(tweet):\n",
    "    # returns it in list format\n",
    "    return [ tweet['created_at'], tweet['retweeted_id'], tweet['og_tweet_id'],\n",
    "            tweet['text'], tweet['screen_name'], tweet['retweet_count'], tweet['favorite_count'],\n",
    "            tweet['lang'], tweet['quote_count'], tweet['reply_count'], tweet['media'], tweet['hashtags'],\n",
    "            tweet['is_retweet']]\n",
    "\n",
    "\n",
    "fields_to_extract = [\n",
    "     'created_at', 'og_tweet_id', 'text', 'screen_name',\n",
    "    'retweet_count', 'favorite_count', 'lang',\n",
    "    'quote_count', 'reply_count', 'media', 'hashtags', 'is_retweet'\n",
    "]\n",
    "\n",
    "def cursor_to_list(cursor, fields):\n",
    "    \"\"\"\n",
    "    Convert MongoDB cursor to a list of lists based on specified fields.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the retweets as lists\n",
    "    retweets_list = []\n",
    "\n",
    "    # Iterate over the retweets cursor\n",
    "    for retweet in cursor:\n",
    "        # Extract the desired fields and append them as a list\n",
    "        retweet_data = [retweet[field] for field in fields]\n",
    "        # Append the retweet data list to the retweets list\n",
    "        retweets_list.append(retweet_data)\n",
    "\n",
    "    # Return the retweets list\n",
    "    return retweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sat Apr 25 12:39:47 +0000 2020', 1254027325475258368, 'RT elmundociencia: Los españoles creen que la \"responsabilidad ciudadana\" es el factor más importante para controlar la epidemia de corona…', 'Jimenezlosantos', 1, 2, 'es', 0, 0, None, [], False], ['Sat Apr 25 12:30:00 +0000 2020', 1254024861715816449, 'வீட்டிலேயே இருப்போம் !\\n\\n#SunNews #Corona #CoronaVirus #Covid19 #Pandemic #Outbreak #stayhome #stayhomestaysafe… https://t.co/A75ae780OT', 'sunnewstamil', 0, 0, 'ta', 0, 0, None, ['SunNews', 'Corona', 'CoronaVirus', 'Covid19', 'Pandemic', 'Outbreak', 'stayhome', 'stayhomestaysafe'], False], ['Sat Apr 25 12:30:00 +0000 2020', 1254024862139510785, 'Update Corona 25 April: 8.607 Kasus, 720 Meninggal, 1.042 Sembuh https://t.co/9QnK4BJ1d3', 'SayHeiYu', 0, 0, 'in', 0, 0, None, [], False], ['Sat Apr 25 12:30:00 +0000 2020', 1254024861204328453, 'हमारे देश के वह न्यूज़ चैनल और एंकर \\n#Corona से भी ज्यादा खतरनाक हैं जो इसे सांप्रदायिक रंग दे रहे है. कहते हैं कि… https://t.co/aGDQ7Aoe4S', 'nlhindi', 0, 0, 'hi', 0, 0, None, ['Corona'], False], ['Sat Apr 25 12:30:00 +0000 2020', 1254024861158002691, 'Al 24 de abril, Puebla ocupa el séptimo lugar nacional de los estados con más casos confirmados de coronavirus en M… https://t.co/8HjnFS6Uq0', 'El_Universal_Mx', 0, 0, 'es', 0, 0, None, [], False]]\n"
     ]
    }
   ],
   "source": [
    "def find_tweet_in_timerange(start_time, end_time, limit, tweet_text):\n",
    "    \n",
    "    start_ts = \"Sat Apr 25 \" + start_time + \" +0000 2020\"\n",
    "    end_ts   = \"Sat Apr 25 \" + end_time   + \" +0000 2020\"\n",
    "\n",
    "#     print(start_ts)\n",
    "#     print(end_ts)\n",
    "#     print(tweet_text)\n",
    "    \n",
    "    # Define the time range query\n",
    "    time_range_query = {\"$match\": {\"created_at\": {\"$gte\": start_ts, \"$lte\": end_ts}}}\n",
    "\n",
    "    # Match tweets containing the specified text\n",
    "    text_query = {\"$match\": {\"text\": {\"$regex\": tweet_text, \"$options\": \"i\"}}}\n",
    "\n",
    "    # Sort by favorite_count in descending order\n",
    "    sort_by_favorite_count = {\"$sort\": {\"favorite_count\": -1}}\n",
    "    \n",
    "    retweet = {\"$match\": {\"is_retweet\" : False}}\n",
    "    \n",
    "    # Limit the number of results\n",
    "    limit_results = {\"$limit\": limit}\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = [\n",
    "        time_range_query,\n",
    "        text_query,\n",
    "        retweet,\n",
    "        sort_by_favorite_count,\n",
    "        limit_results\n",
    "    ]\n",
    "\n",
    "    # Execute the aggregation pipeline\n",
    "    output = tweets_collection.aggregate(pipeline)\n",
    "    \n",
    "    fields_to_extract = [\n",
    "     'created_at', 'og_tweet_id', 'text', 'screen_name',\n",
    "    'retweet_count', 'favorite_count', 'lang',\n",
    "    'quote_count', 'reply_count', 'media', 'hashtags', 'is_retweet'\n",
    "    ]\n",
    "    \n",
    "    tweets_list = cursor_to_list(output, fields_to_extract)\n",
    "\n",
    "    return tweets_list\n",
    "\n",
    "# Example usage:\n",
    "# start_ts = \"Sat Apr 25 11:00:00 +0000 2020\"\n",
    "# end_ts = \"Sat Apr 25 12:30:00 +0000 2020\"\n",
    "start_time = \"12:30:00\"\n",
    "end_time   = \"12:40:00\"\n",
    "tweet_text     = \"corona\"\n",
    "Limit = 5\n",
    "\n",
    "tweets_with_text = find_tweet_in_timerange(start_time, end_time, Limit, tweet_text)\n",
    "print(tweets_with_text)\n",
    "#tweets_list = [dict_to_list(tweet) for tweet in tweets_with_text]\n",
    "#tweets_list\n",
    "\n",
    "# fields_to_extract = [\n",
    "#      'created_at', 'retweeted_id', 'og_tweet_id', 'text', 'screen_name',\n",
    "#     'retweet_count', 'favorite_count', 'lang',\n",
    "#     'quote_count', 'reply_count', 'media', 'hashtags', 'is_retweet'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sat Apr 25 12:39:47 +0000 2020',\n",
       " 1254027325475258368,\n",
       " 'RT elmundociencia: Los españoles creen que la \"responsabilidad ciudadana\" es el factor más importante para controlar la epidemia de corona…',\n",
       " 'Jimenezlosantos',\n",
       " 1,\n",
       " 2,\n",
       " 'es',\n",
       " 0,\n",
       " 0,\n",
       " None,\n",
       " [],\n",
       " False]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes in a start time, end time, limit, and the hashtag we want find\n",
    "and returns all tweets within that timeframe that contains the specified hashtag\n",
    "'''\n",
    "\n",
    "def find_tweet_with_hashtag_in_time(start_time, end_time, limit, hashtag):\n",
    "    \n",
    "    \n",
    "    start_ts = \"Sat Apr 25 \" + start_time + \" +0000 2020\"\n",
    "    end_ts   = \"Sat Apr 25 \" + end_time   + \" +0000 2020\"\n",
    "    # Define the time range query\n",
    "    time_range_query = {\"$match\": {\"created_at\": {\"$gte\": start_ts, \"$lte\": end_ts}}}\n",
    "    # Match tweets containing the specified hashtag\n",
    "    hashtag_match = {\"$match\": {\"hashtags\": hashtag}}\n",
    "    # Sort by favorite_count in descending order\n",
    "    sort_by_favorite_count = {\"$sort\": {\"favorite_count\": -1}}\n",
    "    # Limit the number of results\n",
    "    limit_results = {\"$limit\": limit}\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = [\n",
    "        time_range_query,\n",
    "        hashtag_match,\n",
    "        sort_by_favorite_count,\n",
    "        limit_results\n",
    "    ]\n",
    "    # Execute the aggregation pipeline\n",
    "    output = tweets_collection.aggregate(pipeline)\n",
    "    fields_to_extract = [\n",
    "     'created_at', 'og_tweet_id', 'text', 'screen_name',\n",
    "    'retweet_count', 'favorite_count', 'lang',\n",
    "    'quote_count', 'reply_count', 'media', 'hashtags', 'is_retweet'\n",
    "    ]\n",
    "    \n",
    "    tweets_list = cursor_to_list(output, fields_to_extract)\n",
    "\n",
    "    return tweets_list\n",
    "\n",
    "# start_time = \"12:00:00\"\n",
    "# end_time   = \"12:30:00\"\n",
    "# limit = 10\n",
    "# hashtag = \"25aprile\"\n",
    "start_time = \"12:00:00\"\n",
    "end_time   = \"15:40:00\"\n",
    "hashtag     = \"TriColores\"\n",
    "limit = 5\n",
    "\n",
    "tweets_with_hashtag = find_tweet_with_hashtag_in_time(start_time, end_time, limit, hashtag)\n",
    "\n",
    "# fields_to_extract = [\n",
    "#      'created_at', 'og_tweet_id', 'text', 'screen_name',\n",
    "#     'retweet_count', 'favorite_count', 'lang',\n",
    "#     'quote_count', 'reply_count', 'media', 'hashtags', 'is_retweet'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sat Apr 25 13:56:16 +0000 2020',\n",
       "  1254046571391483905,\n",
       "  '#TriColores #TriCoronas #WillemII #Tilburg #Brabant #CoronaVirusNederland #Covid19NL  13/13\\n\\nhttps://t.co/oF3zNHGUsO',\n",
       "  'ArmoedeZaanstad',\n",
       "  0,\n",
       "  0,\n",
       "  'und',\n",
       "  0,\n",
       "  0,\n",
       "  None,\n",
       "  ['TriColores',\n",
       "   'TriCoronas',\n",
       "   'WillemII',\n",
       "   'Tilburg',\n",
       "   'Brabant',\n",
       "   'CoronaVirusNederland',\n",
       "   'Covid19NL'],\n",
       "  False]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_with_hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sat Apr 04 15:56:50 +0000 2020', 'रामदेव ने पूरी दुनिया से कहा- चीन का किया जाए बहिष्कार\\nhttps://t.co/t4V5gnLhuq', 21517, 4259], ['Wed Apr 22 06:46:51 +0000 2020', 'कोरोना से जंग में पूरी दुनिया ने माना पीएम मोदी की ताकत का लोहा, US सर्वे में टॉप रैंक\\n#coronavirus\\nhttps://t.co/QvYliTGixR', 14119, 1858], ['Fri Apr 24 14:00:05 +0000 2020', 'इलाहाबाद यूनिवर्सिटी के प्रोफेसर मोहम्मद शाहिद को सस्पेंड किया गया \\n#UttarPradesh #TablighiJamat \\nhttps://t.co/5u5EIZi1zr', 2921, 333], ['Thu Apr 23 16:49:10 +0000 2020', 'राहुल गांधी ने फिर उठाया दिहाड़ी मजदूरों का मुद्दा #lockdown \\nhttps://t.co/1bNZ6BlbQD', 2011, 252], ['Sat Apr 25 05:19:58 +0000 2020', 'राहुल के बाद पूर्व प्रधानमंत्री डॉ मनमोहन सिंह भी डीए कटौती के विरोध में | @patelanandk \\n\\nhttps://t.co/fWXaGCV7pk', 1909, 243], ['Fri Apr 24 12:20:51 +0000 2020', 'संक्रमित कपड़ा डालकर नाई ने की लोगों की हजामत, गांव में फैल गया कोरोना... \\n#MadhyaPradesh #Coronavirus \\nhttps://t.co/QLvNPTYtK7', 1823, 415], ['Sat Apr 25 05:08:00 +0000 2020', 'महाराष्ट्र में छिपे हुए सबसे ज्यादा तबलीगी जमाती | @MunishPandeyy \\n\\nhttps://t.co/UNBw9noElT', 1472, 271], ['Wed Apr 22 12:08:44 +0000 2020', '#UttarPradesh: बीजेपी नेता ने #lockdown में कराया क्रिकेट मैच\\n(@ShivendraAajtak)\\n\\nhttps://t.co/3kzdmoxgvu', 1093, 249], ['Sat Apr 25 08:09:11 +0000 2020', '#Lockdown :  गृह मंत्रालय के मुताबिक देश भर में शराब की दुकानें अगले आदेश तक बंद ही रहेंगी | @kamaljitsandhu \\n\\nhttps://t.co/mQQzvzB7rx', 849, 68], ['Sat Apr 25 09:57:07 +0000 2020', 'कपिल सिब्बल का मोदी सरकार पर वार!\\n#Politics\\nhttps://t.co/e8xYSfUYOH', 548, 59]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Both function takes in user screen_name, fields_to_extract (defined above), and returns all the tweets/retweets they sent\n",
    "'''\n",
    "\n",
    "def get_tweets_by_screen_name(screen_name):\n",
    "    # Define the query based on parameters\n",
    "    screen_name_query = {\"$match\": {\"screen_name\": screen_name}}  \n",
    "    sort_by_favorite_count = {\"$sort\": {\"favorite_count\": -1}}\n",
    "    # Limit the number of results\n",
    "    limit_results = {\"$limit\": 10}\n",
    "    retweet = {\"$match\": {\"is_retweet\" : False}}\n",
    "    # Define the pipeline\n",
    "    pipeline = [\n",
    "        screen_name_query,\n",
    "        retweet,\n",
    "        sort_by_favorite_count,\n",
    "        limit_results\n",
    "    ]\n",
    "\n",
    "    # Retrieve top tweets by screen_name\n",
    "    top_tweets_by_screen_name = tweets_collection.aggregate(pipeline)\n",
    "\n",
    "    fields_to_extract = [\"created_at\", \"text\", \"favorite_count\", \"retweet_count\"]\n",
    "    \n",
    "    # Convert MongoDB cursor to list of dictionaries\n",
    "    tweets_list = cursor_to_list(top_tweets_by_screen_name, fields_to_extract)\n",
    "\n",
    "    retweet = {\"$match\": {\"is_retweet\" : True}}\n",
    "    pipeline = [\n",
    "        screen_name_query,\n",
    "        retweet,\n",
    "        sort_by_favorite_count,\n",
    "        limit_results\n",
    "    ]\n",
    "\n",
    "    # Retrieve top tweets by screen_name\n",
    "    top_tweets_by_screen_name = tweets_collection.aggregate(pipeline)\n",
    "\n",
    "    # Convert MongoDB cursor to list of dictionaries\n",
    "    retweets_list = cursor_to_list(top_tweets_by_screen_name, fields_to_extract)\n",
    "\n",
    "    return tweets_list + retweets_list\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "author_screen_name = \"aajtak\"\n",
    "\n",
    "tweets = get_tweets_by_screen_name(author_screen_name)\n",
    "\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sat Apr 25 12:21:44 +0000 2020', 'RT @viewcenterpod: Catch up on latest episode of THE VIEWING CENTER PODCAST where we discussed\\n\\n-The Coronavirus pandemic in Nigeria \\n- TAC…', 'Vikteneus', 0, 0, 'en', 0, 0, None, [], True], ['Sat Apr 25 12:22:33 +0000 2020', 'RT @viewcenterpod: Catch up on latest episode of THE VIEWING CENTER PODCAST where we discussed\\n\\n-The Coronavirus pandemic in Nigeria \\n- TAC…', 'hardeyshurlar1', 0, 0, 'en', 0, 0, None, [], True], ['Sat Apr 25 13:05:41 +0000 2020', 'RT @viewcenterpod: Catch up on latest episode of THE VIEWING CENTER PODCAST where we discussed\\n\\n-The Coronavirus pandemic in Nigeria \\n- TAC…', 'Omegsb', 0, 0, 'en', 0, 0, None, [], True], ['Sat Apr 25 13:34:42 +0000 2020', 'RT @viewcenterpod: Catch up on latest episode of THE VIEWING CENTER PODCAST where we discussed\\n\\n-The Coronavirus pandemic in Nigeria \\n- TAC…', '_KHAAMA_', 0, 0, 'en', 0, 0, None, [], True]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This function takes in the original tweet id and returns every retweeter that has retweeted the input tweet id. \n",
    "'''\n",
    "\n",
    "def find_retweeters_by_tweet_id(tweet_id):\n",
    "    # Find the original tweet using its og_tweet_id\n",
    "    original_tweet = tweets_collection.find({'og_tweet_id': tweet_id, 'is_retweet': True})\n",
    "    fields_to_extract = [\n",
    "     'created_at', 'text', 'screen_name',\n",
    "    'retweet_count', 'favorite_count', 'lang',\n",
    "    'quote_count', 'reply_count', 'media', 'hashtags', 'is_retweet'\n",
    "    ]\n",
    "    \n",
    "    tweets_list = cursor_to_list(original_tweet, fields_to_extract)\n",
    "\n",
    "    return tweets_list\n",
    "\n",
    "original_tweet_id = 1254020851877453825\n",
    "retweeters = find_retweeters_by_tweet_id(original_tweet_id)\n",
    "\n",
    "print(retweeters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start_time = \"12:30:00\"\n",
    "# end_time   = \"12:40:00\"\n",
    "# string     = \"Corona\"\n",
    "# limit = 5\n",
    "\n",
    "def query_tweet_string(start_time, end_time, string, limit):\n",
    "    \n",
    "    start_ts = \"Sat Apr 25 \" + start_time + \" +0000 2020\"\n",
    "    end_ts   = \"Sat Apr 25 \" + end_time   + \" +0000 2020\"\n",
    "\n",
    "    print(start_ts)\n",
    "    print(end_ts)\n",
    "    print(string)\n",
    "\n",
    "    time_range_query = {\"$match\": {\"created_at\": {\"$gte\": start_ts,\"$lte\": end_ts}}}\n",
    "    text_query = {\"$match\": {\"text\": {\"$regex\": string, \"$options\": \"i\"}}}\n",
    "    retweet = {\"$match\": {\"is_retweet\" : False}}\n",
    "    order = { \"$sort\": { \"favorite_count\": 1 } }\n",
    "    count = { \"$limit\": limit } \n",
    "\n",
    "    # Define the pipeline using the variables\n",
    "    pipeline = [\n",
    "        time_range_query,\n",
    "        text_query,\n",
    "        retweet,\n",
    "        order,\n",
    "        count\n",
    "    ]\n",
    "\n",
    "    output = tweets_collection.aggregate(pipeline)\n",
    "\n",
    "    result = []\n",
    "    for document in output:\n",
    "        result.append(document)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook MongoDB_Load_Data.ipynb to script\n",
      "[NbConvertApp] Writing 16073 bytes to MongoDB_Load_Data.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script MongoDB_Load_Data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db.hashtags.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db.users.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db.tweets.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
